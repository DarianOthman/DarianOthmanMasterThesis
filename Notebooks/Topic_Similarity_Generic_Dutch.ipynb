{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "from scipy.spatial.distance import cosine"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T18:02:58.413638300Z",
     "start_time": "2024-05-19T18:02:58.398016800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "api_key = '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'\n",
    "\n",
    "def get_embeddings(text):\n",
    "\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    data = {\n",
    "        \"input\": text,\n",
    "        \"model\": \"text-embedding-3-large\"\n",
    "    }\n",
    "    response = requests.post(\"https://api.openai.com/v1/embeddings\", headers=headers, json=data)\n",
    "    if response.status_code == 200:\n",
    "        embedding = response.json()['data'][0]['embedding']\n",
    "        return np.array(embedding)\n",
    "    else:\n",
    "        print(f\"Error with text: {text[:30]}... Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def similarity(text_embeddings_real, text_embeddings_synthetic):\n",
    "    average_similarities = []\n",
    "    for i, group1_embeddings in enumerate(text_embeddings_real):\n",
    "        for j, group2_embeddings in enumerate(text_embeddings_synthetic):\n",
    "            # Convert embeddings to numpy arrays\n",
    "            group1_embeddings = np.array(group1_embeddings)\n",
    "            group2_embeddings = np.array(group2_embeddings)\n",
    "            \n",
    "            # Calculate cosine similarities between corresponding embeddings\n",
    "            similarity_matrix = cosine_similarity(group1_embeddings, group2_embeddings)\n",
    "            \n",
    "            # Calculate average similarity\n",
    "            average_similarity = np.mean(similarity_matrix)\n",
    "            \n",
    "            # Store the average similarity\n",
    "            average_similarities.append(average_similarity)\n",
    "            \n",
    "    max_similarity = np.percentile(average_similarities, 75)\n",
    "    return max_similarity, average_similarities"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T17:51:14.140227400Z",
     "start_time": "2024-05-19T17:51:14.124606700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "file_path_synthetic = 'C:/Users/I6240624/Documents/BISS/Master Thesis/Code/DarianOthmanMasterThesis/Topic_Dutch/Generic_t=1_P=1_Instagram_Dutch_topics.csv'\n",
    "\n",
    "\n",
    "data_synthetic = pd.read_csv(file_path_synthetic, sep=';')\n",
    "topics_synthetic = data_synthetic[\"Representation\"].tolist()\n",
    "topics_synthetic = [eval(list_str) for list_str in topics_synthetic]\n",
    "\n",
    "\n",
    "text_embeddings_synthetic_ins = [[get_embeddings(word) for word in sentence] for sentence in topics_synthetic]\n",
    "with open('C:/Users/I6240624/Documents/BISS/Master Thesis/Code/DarianOthmanMasterThesis/Topic_Dutch/Generic_t=1_P=1_Instagram_Dutch_topics_emb.pkl', 'wb') as f:\n",
    "    pickle.dump(text_embeddings_synthetic_ins, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T18:07:42.737799700Z",
     "start_time": "2024-05-19T18:06:23.114442200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "file_path_real = 'C:/Users/I6240624/Documents/BISS/Master Thesis/Code/DarianOthmanMasterThesis/Topic_Dutch/Generic_t=1_P=1_Instagram_Dutch_real_topics.csv'\n",
    "\n",
    "\n",
    "data_real = pd.read_csv(file_path_real, sep=';')\n",
    "topics_real = data_real[\"Representation\"].tolist()\n",
    "topics_real = [eval(list_str) for list_str in topics_real]\n",
    "\n",
    "\n",
    "text_embeddings_real_ins = [[get_embeddings(word) for word in sentence] for sentence in topics_real]\n",
    "with open('C:/Users/I6240624/Documents/BISS/Master Thesis/Code/DarianOthmanMasterThesis/Topic_Dutch/Generic_t=1_P=1_Instagram_Dutch_real_topics_emb.pkl', 'wb') as f:\n",
    "    pickle.dump(text_embeddings_real_ins, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T18:10:24.610453300Z",
     "start_time": "2024-05-19T18:08:26.517543900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "file_path_synthetic = 'C:/Users/I6240624/Documents/BISS/Master Thesis/Code/DarianOthmanMasterThesis/Topic_Dutch/Generic_t=1_P=1_YouTube_Dutch_topics.csv'\n",
    "\n",
    "\n",
    "data_synthetic = pd.read_csv(file_path_synthetic, sep=';')\n",
    "topics_synthetic = data_synthetic[\"Representation\"].tolist()\n",
    "topics_synthetic = [eval(list_str) for list_str in topics_synthetic]\n",
    "\n",
    "\n",
    "text_embeddings_synthetic_yt = [[get_embeddings(word) for word in sentence] for sentence in topics_synthetic]\n",
    "with open('C:/Users/I6240624/Documents/BISS/Master Thesis/Code/DarianOthmanMasterThesis/Topic_Dutch/Generic_t=1_P=1_YouTube_Dutch_topics_emb.pkl', 'wb') as f:\n",
    "    pickle.dump(text_embeddings_synthetic_yt, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T18:13:10.353703400Z",
     "start_time": "2024-05-19T18:10:24.610453300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "file_path_real = 'C:/Users/I6240624/Documents/BISS/Master Thesis/Code/DarianOthmanMasterThesis/Topic_Dutch/Generic_t=1_P=1_YouTube_Dutch_real_topics.csv'\n",
    "\n",
    "\n",
    "data_real = pd.read_csv(file_path_real, sep=';')\n",
    "topics_real = data_real[\"Representation\"].tolist()\n",
    "topics_real = [eval(list_str) for list_str in topics_real]\n",
    "\n",
    "\n",
    "text_embeddings_real_yt = [[get_embeddings(word) for word in sentence] for sentence in topics_real]\n",
    "with open('C:/Users/I6240624/Documents/BISS/Master Thesis/Code/DarianOthmanMasterThesis/Topic_Dutch/Generic_t=1_P=1_YouTube_Dutch_real_topics_emb.pkl', 'wb') as f:\n",
    "    pickle.dump(text_embeddings_real_yt, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T18:15:16.149163300Z",
     "start_time": "2024-05-19T18:13:10.353703400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "file_path_synthetic = 'C:/Users/I6240624/Documents/BISS/Master Thesis/Code/DarianOthmanMasterThesis/Topic_Dutch/Generic_t=1_P=1_TikTok_Dutch_topics.csv'\n",
    "\n",
    "\n",
    "data_synthetic = pd.read_csv(file_path_synthetic, sep=';')\n",
    "topics_synthetic = data_synthetic[\"Representation\"].tolist()\n",
    "topics_synthetic = [eval(list_str) for list_str in topics_synthetic]\n",
    "\n",
    "\n",
    "text_embeddings_synthetic_tt = [[get_embeddings(word) for word in sentence] for sentence in topics_synthetic]\n",
    "with open('C:/Users/I6240624/Documents/BISS/Master Thesis/Code/DarianOthmanMasterThesis/Topic_Dutch/Generic_t=1_P=1_TikTok_Dutch_topics_emb.pkl', 'wb') as f:\n",
    "    pickle.dump(text_embeddings_synthetic_tt, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T18:16:58.635029300Z",
     "start_time": "2024-05-19T18:15:16.149163300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "file_path_real = 'C:/Users/I6240624/Documents/BISS/Master Thesis/Code/DarianOthmanMasterThesis/Topic_Dutch/Generic_t=1_P=1_TikTok_Dutch_real_topics.csv'\n",
    "\n",
    "\n",
    "data_real = pd.read_csv(file_path_real, sep=';')\n",
    "topics_real = data_real[\"Representation\"].tolist()\n",
    "topics_real = [eval(list_str) for list_str in topics_real]\n",
    "\n",
    "\n",
    "text_embeddings_real_tt = [[get_embeddings(word) for word in sentence] for sentence in topics_real]\n",
    "with open('C:/Users/I6240624/Documents/BISS/Master Thesis/Code/DarianOthmanMasterThesis/Topic_Dutch/Generic_t=1_P=1_TikTok_Dutch_real_topics_emb.pkl', 'wb') as f:\n",
    "    pickle.dump(text_embeddings_real_tt, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T22:03:37.469738900Z",
     "start_time": "2024-05-19T22:02:26.423365900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (10,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[120], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m np\u001B[38;5;241m.\u001B[39mpercentile((similarity(text_embeddings_real_ins, text_embeddings_synthetic_ins)[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39msimilarity(text_embeddings_real_yt, text_embeddings_synthetic_yt)[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[43msimilarity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext_embeddings_real_tt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext_embeddings_synthetic_tt\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m1\u001B[39m]),\u001B[38;5;241m75\u001B[39m)\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#similarity(text_embeddings_real_tt, text_embeddings_synthetic_tt)[0]\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[95], line 26\u001B[0m, in \u001B[0;36msimilarity\u001B[1;34m(text_embeddings_real, text_embeddings_synthetic)\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, group1_embeddings \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(text_embeddings_real):\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m j, group2_embeddings \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(text_embeddings_synthetic):\n\u001B[0;32m     25\u001B[0m         \u001B[38;5;66;03m# Convert embeddings to numpy arrays\u001B[39;00m\n\u001B[1;32m---> 26\u001B[0m         group1_embeddings \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgroup1_embeddings\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m         group2_embeddings \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(group2_embeddings)\n\u001B[0;32m     29\u001B[0m         \u001B[38;5;66;03m# Calculate cosine similarities between corresponding embeddings\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (10,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "np.percentile((similarity(text_embeddings_real_ins, text_embeddings_synthetic_ins)[1] +similarity(text_embeddings_real_yt, text_embeddings_synthetic_yt)[1] + similarity(text_embeddings_real_tt, text_embeddings_synthetic_tt)[1]),75)\n",
    "#similarity(text_embeddings_real_tt, text_embeddings_synthetic_tt)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T18:18:07.784051800Z",
     "start_time": "2024-05-19T18:18:07.091272900Z"
    }
   }
  }
 ]
}
